######################################################
#          conservative_sac_exploitation.py          #
#              Trains the last O of OOO              #
#        Exploitation training via Offline RL        #
######################################################


import os
import numpy as np
import gym
import d4rl  # noqa: F401
import absl.app
import absl.flags
from tqdm import tqdm

from .conservative_sac import ConservativeSAC
from .replay_buffer import (
    subsample_batch,
    concatenate_batches,
    get_d4rl_dataset_with_mc_calculation,
    get_hand_dataset_with_mc_calculation,
)
from .jax_utils import batch_to_jax
from .model import TanhGaussianPolicy, FullyConnectedQFunction, SamplerPolicy
from .sampler import TrajSampler
from .utils import (
    Timer,
    define_flags_with_default,
    set_random_seed,
    get_user_flags,
    prefix_metrics,
    WandBLogger,
)
from viskit.logging import logger, setup_logger
from .replay_buffer import ReplayBuffer


FLAGS_DEF = define_flags_with_default(
    exp_name="debugging",
    env="antmaze-medium-diverse-v2",
    seed=42,
    save_model=False,
    batch_size=256,
    reward_scale=1.0,
    reward_bias=0.0,
    clip_action=0.99999,
    policy_arch="256-256",
    qf_arch="256-256",
    orthogonal_init=True,
    policy_log_std_multiplier=1.0,
    policy_log_std_offset=-1.0,
    # Total grad_steps of offline pretrain will be (n_train_step_per_epoch_offline * n_pretrain_epochs)
    n_train_step_per_epoch_offline=1000,
    n_pretrain_epochs=1000,
    offline_eval_every_n_epoch=50,
    eval_n_trajs=100,
    replay_buffer_size=2000000,
    mixing_ratio=-1.0,
    use_cql=True,
    cql_min_q_weight=5.0,
    enable_calql=False,  # Turn on for Cal-QL
    cql=ConservativeSAC.get_default_config(),
    logging=WandBLogger.get_default_config(),
    # For GURL exploration environments:
    dataset_path="",
    max_len=-1,
    exploitation_timestep=-1,
    replay_buffer_path="",
    replay_buffer_original_bias=0.0,
    replay_buffer_original_scale=1.0,
    bound_q_functions_according_to_data=False,
    debugging=False,
    upsample_batch_size=0,
)


def main(argv):
    FLAGS = absl.flags.FLAGS
    variant = get_user_flags(FLAGS, FLAGS_DEF)
    assert FLAGS.exploitation_timestep > 0
    assert os.path.exists(FLAGS.replay_buffer_path)
    wandb_config = FLAGS.logging
    wandb_config.experiment_id = FLAGS.exp_name
    wandb_logger = WandBLogger(config=wandb_config, variant=variant)
    setup_logger(
        variant=variant,
        exp_id=wandb_logger.experiment_id,
        seed=FLAGS.seed,
        base_log_dir=FLAGS.logging.output_dir,
        include_exp_prefix_sub_dir=False,
    )

    if FLAGS.env in ["pen-binary-v0", "door-binary-v0", "relocate-binary-v0"]:
        import mj_envs  # noqa: F401

        dataset = get_hand_dataset_with_mc_calculation(
            FLAGS.env,
            gamma=FLAGS.cql.discount,
            reward_scale=FLAGS.reward_scale,
            reward_bias=FLAGS.reward_bias,
            clip_action=FLAGS.clip_action,
        )
        use_goal = True
    else:
        dataset = get_d4rl_dataset_with_mc_calculation(
            FLAGS.env,
            FLAGS.reward_scale,
            FLAGS.reward_bias,
            FLAGS.clip_action,
            gamma=FLAGS.cql.discount,
            dataset_path=FLAGS.dataset_path,
            max_len=FLAGS.max_len,
        )
        use_goal = False

    if dataset is not None:
        assert dataset["next_observations"].shape == dataset["observations"].shape
    else:
        print("------ Important: No dataset is used. ------")

    set_random_seed(FLAGS.seed)
    eval_env = gym.make(FLAGS.env).unwrapped
    eval_sampler = TrajSampler(eval_env, use_goal, gamma=FLAGS.cql.discount)

    replay_buffer = ReplayBuffer(
        FLAGS.replay_buffer_size,
    )
    replay_buffer.load(
        FLAGS.replay_buffer_path,
        env_name=FLAGS.env,
        gamma=FLAGS.cql.discount,
        reward_scale=FLAGS.reward_scale,
        reward_bias=FLAGS.reward_bias,
        max_steps=FLAGS.exploitation_timestep,
        clip_action=FLAGS.clip_action,
    )
    assert (
        replay_buffer.total_steps == FLAGS.exploitation_timestep
    ), f"{replay_buffer.total_steps} != {FLAGS.exploitation_timestep}"

    # Adjust the rewards based on the original bias/scale parameters and the new ones
    replay_buffer._rewards = (
        replay_buffer._rewards - FLAGS.replay_buffer_original_bias
    ) / FLAGS.replay_buffer_original_scale
    replay_buffer._rewards = (
        replay_buffer._rewards * FLAGS.reward_scale + FLAGS.reward_bias
    )

    if FLAGS.upsample_batch_size > 0:
        if "hammer" in FLAGS.env:
            max_reward = 25.0 * FLAGS.reward_scale + FLAGS.reward_bias
        else:
            max_reward = replay_buffer._rewards.max()
        positive_transition_indices = np.where(replay_buffer._rewards >= max_reward)[0]
        upsample_replay_buffer = ReplayBuffer(
            positive_transition_indices.shape[0],
        )
        upsample_replay_buffer.add_batch(
            replay_buffer.select(positive_transition_indices)
        )
        assert (
            upsample_replay_buffer.total_steps == positive_transition_indices.shape[0]
        )

    observation_dim = eval_sampler.env.observation_space.shape[0]
    action_dim = eval_sampler.env.action_space.shape[0]

    policy = TanhGaussianPolicy(
        observation_dim,
        action_dim,
        FLAGS.policy_arch,
        FLAGS.orthogonal_init,
        FLAGS.policy_log_std_multiplier,
        FLAGS.policy_log_std_offset,
    )

    qf = FullyConnectedQFunction(
        observation_dim, action_dim, FLAGS.qf_arch, FLAGS.orthogonal_init
    )

    if FLAGS.bound_q_functions_according_to_data:
        dataset_rewards = [np.inf] if dataset is None else dataset["rewards"]
        min_reward = min(np.min(dataset_rewards), np.min(replay_buffer.data["rewards"]))
        dataset_rewards = [-np.inf] if dataset is None else dataset["rewards"]
        max_reward = max(np.max(dataset_rewards), np.max(replay_buffer.data["rewards"]))
        min_q = min_reward / (1.0 - FLAGS.cql.discount)
        max_q = max_reward / (1.0 - FLAGS.cql.discount)
    elif FLAGS.cql.bound_q_functions:
        min_q = FLAGS.cql.min_reward / (1.0 - FLAGS.cql.discount)
        max_q = FLAGS.cql.max_reward / (1.0 - FLAGS.cql.discount)
    else:
        min_q = None
        max_q = None

    if FLAGS.cql.target_entropy >= 0.0:
        FLAGS.cql.target_entropy = -np.prod(eval_sampler.env.action_space.shape).item()

    sac = ConservativeSAC(FLAGS.cql, policy, qf)
    sampler_policy = SamplerPolicy(sac.policy, sac.train_params["policy"])

    viskit_metrics = {}
    n_train_step_per_epoch = FLAGS.n_train_step_per_epoch_offline
    cql_min_q_weight = FLAGS.cql_min_q_weight
    enable_calql = FLAGS.enable_calql
    use_cql = FLAGS.use_cql
    mixing_ratio = FLAGS.mixing_ratio

    total_grad_steps = 0
    do_eval = False
    train_timer = None
    epoch = 0
    train_metrics = None
    while True:
        metrics = {"epoch": epoch}

        """
        Do evaluations when
        1. epoch = 0 to get initial performance
        2. every FLAGS.offline_eval_every_n_epoch for offline phase
        3. epoch == FLAGS.n_pretrain_epochs to get offline pre-trained performance
        4. every FLAGS.online_eval_every_n_env_steps for online phase
        5. when replay_buffer.total_steps >= FLAGS.max_online_env_steps to get final fine-tuned performance
        """
        do_eval = (
            epoch == 0
            or (epoch % FLAGS.offline_eval_every_n_epoch == 0)
            or (epoch == FLAGS.n_pretrain_epochs)
        )

        with Timer() as eval_timer:
            if do_eval:
                print(f"Starting Evaluation for Epoch {epoch}")
                trajs = eval_sampler.sample(
                    sampler_policy.update_params(sac.train_params["policy"]),
                    FLAGS.eval_n_trajs,
                    deterministic=True,
                )

                metrics["evaluation/average_return"] = np.mean(
                    [np.sum(t["rewards"]) for t in trajs]
                )
                metrics["evaluation/average_traj_length"] = np.mean(
                    [len(t["rewards"]) for t in trajs]
                )
                if use_goal:
                    # for adroit envs
                    metrics["evaluation/goal_achieved_rate"] = np.mean(
                        [1 in t["goal_achieved"] for t in trajs]
                    )
                else:
                    # for d4rl envs
                    if (
                        "get_normalized_score" in dir(eval_sampler.env)
                        and FLAGS.env != "hammer-v0"
                    ):
                        metrics["evaluation/average_normalized_return"] = np.mean(
                            [
                                eval_sampler.env.get_normalized_score(
                                    np.sum(t["rewards"])
                                )
                                for t in trajs
                            ]
                        )

                if "kitchen" in FLAGS.env:
                    metrics["evaluation/num_stages_solved"] = np.mean(
                        [t["rewards"][-1] for t in trajs]
                    )
                    metrics["evaluation/num_stages_solved_normalized"] = (
                        np.mean([t["rewards"][-1] for t in trajs]) / 4
                    )
                # Log the average of max rewards over episode
                metrics["evaluation/max_reward"] = np.mean(
                    [np.max(t["rewards"]) for t in trajs]
                )

                if FLAGS.save_model:
                    save_data = {"sac": sac, "variant": variant, "epoch": epoch}
                    wandb_logger.save_pickle(save_data, "model.pkl")

        metrics["grad_steps"] = total_grad_steps

        metrics["epoch"] = epoch
        metrics["train_time"] = 0 if train_timer is None else train_timer()
        metrics["eval_time"] = eval_timer()
        metrics["epoch_time"] = (
            eval_timer() if train_timer is None else train_timer() + eval_timer()
        )
        if FLAGS.n_pretrain_epochs >= 0:
            metrics["mixing_ratio"] = mixing_ratio

        metrics["min_q"] = min_q
        metrics["max_q"] = max_q
        if train_metrics is not None:
            metrics.update(train_metrics)

        wandb_logger.log(metrics)
        viskit_metrics.update(metrics)
        logger.record_dict(viskit_metrics)
        logger.dump_tabular(with_prefix=False, with_timestamp=False)

        if epoch == FLAGS.n_pretrain_epochs:
            print("Finished training")
            break

        if train_timer is None:
            print("jit compiling train function: will take a while")

        with Timer() as train_timer:
            if FLAGS.n_pretrain_epochs >= 0:
                if FLAGS.mixing_ratio >= 0:
                    mixing_ratio = FLAGS.mixing_ratio
                else:
                    mixing_ratio = dataset["rewards"].shape[0] / (
                        dataset["rewards"].shape[0] + replay_buffer.total_steps
                    )
                batch_size_offline = int(FLAGS.batch_size * mixing_ratio)
                batch_size_online = FLAGS.batch_size - batch_size_offline

            for _ in tqdm(range(n_train_step_per_epoch)):
                # mix offline and online buffer
                online_batch = replay_buffer.sample(batch_size_online)
                if FLAGS.upsample_batch_size > 0:
                    upsample_batch = upsample_replay_buffer.sample(
                        FLAGS.upsample_batch_size
                    )
                    online_batch = concatenate_batches([online_batch, upsample_batch])
                if dataset is not None:
                    offline_batch = subsample_batch(dataset, batch_size_offline)
                    batch = concatenate_batches([offline_batch, online_batch])
                else:
                    batch = online_batch
                if FLAGS.debugging:
                    import pdb

                    pdb.set_trace()
                """
                Kitchen dataset has terminations when the demonstrations end. Using this is
                incorrect, because we do want to bootstrap from the final state of the
                demonstration. We modify the data to only terminate when the 4 tasks are completed.
                """
                if "kitchen" in FLAGS.env:
                    batch["dones"] = (batch["rewards"] == 0).astype(np.float32)
                batch = batch_to_jax(batch)

                train_metrics = prefix_metrics(
                    sac.train(
                        batch,
                        use_cql=use_cql,
                        cql_min_q_weight=cql_min_q_weight,
                        enable_calql=enable_calql,
                        bound_q_functions=FLAGS.bound_q_functions_according_to_data
                        or FLAGS.cql.bound_q_functions,
                        min_q=min_q,
                        max_q=max_q,
                    ),
                    "sac",
                )
            total_grad_steps += n_train_step_per_epoch
        epoch += 1


if __name__ == "__main__":
    absl.app.run(main)
