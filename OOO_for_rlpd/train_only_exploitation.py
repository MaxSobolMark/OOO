######################################################
#             train_only_exploitation.py             #
#              Trains the last O of OOO              #
#        Exploitation training via Offline RL        #
######################################################
import os
from pathlib import Path
import pickle

import d4rl
import d4rl.gym_mujoco
import d4rl.locomotion  # noqa: F401 (register environments with gym)
import numpy as np
import tqdm
from absl import app, flags

try:
    from flax.training import checkpoints
except:
    print("Not loading checkpointing functionality.")
from ml_collections import config_flags

import wandb
from rlpd.agents import SACLearner

from rlpd.evaluation import evaluate
from rlpd.env_utils import make_env_and_dataset
from rlpd.dataset_utils import Dataset, Batch

FLAGS = flags.FLAGS

flags.DEFINE_string("project_name", "rlpd", "wandb project name.")
flags.DEFINE_string("exp_name", None, "Name of experiment.")
flags.DEFINE_string("env_name", "halfcheetah-expert-v2", "D4rl dataset name.")
flags.DEFINE_integer("seed", 42, "Random seed.")
flags.DEFINE_integer("eval_episodes", 100, "Number of episodes used for evaluation.")
flags.DEFINE_integer("log_interval", 1000, "Logging interval.")
flags.DEFINE_integer("eval_interval", 50000, "Eval interval.")
flags.DEFINE_integer("batch_size", 256, "Mini batch size.")
flags.DEFINE_integer("max_steps", int(1e6), "Number of training steps.")
flags.DEFINE_integer("buffer_start_index", 0, "start index from the dumped buffer")
flags.DEFINE_integer("buffer_end_index", int(1e6), "end index from the dumped buffer")

flags.DEFINE_boolean("tqdm", True, "Use tqdm progress bar.")
flags.DEFINE_boolean("save_video", False, "Save videos during evaluation.")
flags.DEFINE_boolean("checkpoint_model", True, "Save agent checkpoint on evaluation.")
flags.DEFINE_integer("utd_ratio", 1, "Update to data ratio.")
flags.DEFINE_boolean(
    "binary_include_bc", True, "Whether to include BC data in the binary datasets."
)

config_flags.DEFINE_config_file(
    "config",
    "OOO_for_rlpd/configs/sac_config.py",
    "File path to the training hyperparameter configuration.",
    lock_config=False,
)
flags.DEFINE_string("dataset_path", None, "Path to dataset.")
flags.DEFINE_integer(
    "max_len", None, "Trim every trajectory in offline dataset to this length."
)
flags.DEFINE_integer(
    "upsample_batch_size", 0, "Upsampling batches only use positive transitions."
)
flags.DEFINE_boolean("bound_q_functions_according_to_data", False, "Bound Q functions.")
flags.DEFINE_boolean("ant_sparse_include_xpos_in_obs", False, "Include xpos in obs.")


def combine(one_dict, other_dict):
    combined = {}

    for k, v in one_dict.items():
        if v is None:
            combined[k] = other_dict[k]
        elif isinstance(v, dict):
            combined[k] = combine(v, other_dict[k])
        else:
            tmp = np.empty(
                (v.shape[0] + other_dict[k].shape[0], *v.shape[1:]), dtype=v.dtype
            )
            tmp[: v.shape[0]] = v
            tmp[v.shape[0] :] = other_dict[k]
            combined[k] = tmp

    return combined


def load_dataset(dataset_path, start_index, end_index):
    file = open(dataset_path, "rb")
    data = pickle.load(file)
    file.close()

    for key in data:
        num_data = data[key].shape[0]
        assert start_index >= 0 and end_index <= num_data and end_index > start_index
        data[key] = data[key][start_index:end_index]

    print()
    for key in data:
        print("Key: ", key, " data size: ", data[key].shape)
    ds = Dataset(
        data["observations"],
        data["actions"],
        data["rewards"],
        data["masks"],
        data["dones"],
        data["next_observations"],
        data["observations"].shape[0],
    )
    return ds


def main(_):
    wandb.init(
        project=FLAGS.project_name,
        settings=wandb.Settings(start_method="fork", _service_wait=300),
        name=FLAGS.exp_name,
    )
    wandb.config.update(FLAGS)

    exp_prefix = f"s{FLAGS.seed}_{FLAGS.max_steps}max_steps"
    if hasattr(FLAGS.config, "critic_layer_norm") and FLAGS.config.critic_layer_norm:
        exp_prefix += "_LN"

    log_dir = f"./results/{FLAGS.exp_name}/{FLAGS.seed}"
    os.makedirs(log_dir, exist_ok=True)

    if FLAGS.checkpoint_model:
        chkpt_dir = os.path.join(log_dir, "checkpoints")
        os.makedirs(chkpt_dir, exist_ok=True)

    env, _, eval_env = make_env_and_dataset(
        FLAGS.env_name,
        FLAGS.seed,
        include_bc_data=FLAGS.binary_include_bc,
        max_len=FLAGS.max_len,
        dataset_path=FLAGS.dataset_path,
    )

    # Load dataset, use up to buffer_end_index
    ds = load_dataset(
        dataset_path=FLAGS.dataset_path,
        start_index=FLAGS.buffer_start_index,
        end_index=FLAGS.buffer_end_index,
    )
    if FLAGS.upsample_batch_size > 0:
        max_reward = ds.rewards.max()
        positive_transition_indices = np.where(ds.rewards == max_reward)[0]
        upsample_ds = Dataset(
            ds.observations[positive_transition_indices],
            ds.actions[positive_transition_indices],
            ds.rewards[positive_transition_indices],
            ds.masks[positive_transition_indices],
            ds.dones_float[positive_transition_indices],
            ds.next_observations[positive_transition_indices],
            positive_transition_indices.shape[0],
        )

    if FLAGS.bound_q_functions_according_to_data:
        max_reward = ds.rewards.max()
        min_reward = ds.rewards.min()
        max_q = max_reward / (1 - FLAGS.config.discount)
        min_q = min_reward / (1 - FLAGS.config.discount)
        wandb.log(
            {
                "dataset/max_reward": max_reward,
                "dataset/min_reward": min_reward,
                "dataset/max_q": max_q,
                "dataset/min_q": min_q,
            }
        )
    else:
        max_q = None
        min_q = None
    kwargs = dict(FLAGS.config)
    model_cls = kwargs.pop("model_cls")
    agent = globals()[model_cls].create(
        FLAGS.seed, env.observation_space, env.action_space, **kwargs
    )

    for i in tqdm.tqdm(
        range(0, FLAGS.max_steps + 1), smoothing=0.1, disable=not FLAGS.tqdm
    ):
        batch = ds.sample(int(FLAGS.batch_size * FLAGS.utd_ratio))
        if FLAGS.upsample_batch_size > 0:
            upsample_batch = upsample_ds.sample(FLAGS.upsample_batch_size)
            batch = Batch(**combine(batch.__dict__, upsample_batch.__dict__))

        if "antmaze" in FLAGS.env_name:
            batch["rewards"] -= 1
        elif "kitchen" in FLAGS.env_name:
            batch["masks"] = (batch["rewards"] != 4.0).astype(np.float32)
            reward_bias = -5 if FLAGS.config.get("use_rnd", False) else -4
            batch["rewards"] += reward_bias

        agent, update_info = agent.update(
            batch.__dict__,
            FLAGS.utd_ratio,
            use_rnd=FLAGS.config.get("use_rnd", False),
            max_q=max_q,
            min_q=min_q,
        )

        if i % FLAGS.log_interval == 0:
            for k, v in update_info.items():
                wandb.log({f"training/{k}": v}, step=i)

        if i % FLAGS.eval_interval == 0:
            eval_info = evaluate(
                agent,
                eval_env,
                num_episodes=FLAGS.eval_episodes,
                save_video=FLAGS.save_video,
            )

            for k, v in eval_info.items():
                wandb.log({f"evaluation/{k}": v}, step=i)

            if FLAGS.checkpoint_model:
                try:
                    checkpoints.save_checkpoint(
                        chkpt_dir, agent, step=i, keep=20, overwrite=True
                    )
                except Exception as e:
                    print("Could not save model checkpoint.")


if __name__ == "__main__":
    app.run(main)
