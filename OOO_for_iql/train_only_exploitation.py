######################################################
#             train_only_exploitation.py             #
#              Trains the last O of OOO              #
#        Exploitation training via Offline RL        #
######################################################

import os
import numpy as np
import tqdm
from absl import app, flags
from ml_collections import config_flags, ConfigDict
import wandb

from dataset_utils import Batch, Dataset, ReplayBuffer
from evaluation import evaluate
from learner import Learner
from env_utils import (
    make_env_and_dataset,
    load_replay_buffer,
    get_kitchen_successful_samples,
)

FLAGS = flags.FLAGS

flags.DEFINE_string("exp_name", None, "Experiment name")
flags.DEFINE_string("env_name", "point_mass_wall", "Environment name.")
flags.DEFINE_integer(
    "max_len", None, "Trim every trajectory in offline dataset to this length."
)
flags.DEFINE_integer("seed", 42, "Random seed.")
flags.DEFINE_integer("eval_episodes", 100, "Number of episodes used for evaluation.")
flags.DEFINE_integer("log_interval", 1000, "Logging interval.")
flags.DEFINE_integer("eval_interval", 50000, "Eval interval for each timestep.")
flags.DEFINE_list(
    "online_eval_timesteps",
    None,
    "Timesteps to evaluate at. Example: [500000, 1000000]",
)
flags.DEFINE_integer("batch_size", 256, "Mini batch size.")
flags.DEFINE_integer("max_steps", int(1e6), "Number of training steps.")
flags.DEFINE_boolean("tqdm", True, "Use tqdm progress bar.")
config_flags.DEFINE_config_file(
    "config",
    "configs/exploitation_config.py",
    "File path to the training hyperparameter configuration.",
    lock_config=False,
)
flags.DEFINE_string("replay_buffer_path", None, "Path to replay buffer.")
flags.DEFINE_integer("offline_dataset_size", None, "Size of offline dataset.")
flags.DEFINE_integer("rewards_bias", 0, "Rewards bias.")
flags.DEFINE_integer("rewards_scale", 1, "Rewards scale.")
flags.DEFINE_string(
    "load_rnd_from_path",
    None,
    "Path to load RND nets from. This is only used for the frozen RND experiments.",
)
flags.DEFINE_boolean("use_rnd", False, "Use RND intrinsic reward.")
flags.DEFINE_float("intrinsic_reward_scale", 0, "Intrinsic reward scale")


def main(_):
    print("Environment name: ", FLAGS.env_name)
    assert FLAGS.exp_name is not None
    assert FLAGS.replay_buffer_path is not None
    assert FLAGS.offline_dataset_size is not None
    assert (
        FLAGS.online_eval_timesteps is not None and len(FLAGS.online_eval_timesteps) > 0
    )
    # Put all flags into a dictionary for logging
    flags_dict = FLAGS.flag_values_dict()
    flags_dict["config"] = {
        k: v if not isinstance(v, ConfigDict) else v.to_dict()
        for k, v in FLAGS.config.items()
    }

    wandb.init(
        project="OOO-iql",
        settings=wandb.Settings(start_method="fork", _service_wait=300),
        name=FLAGS.exp_name,
        config=flags_dict,
    )
    print("Experiment Name:", FLAGS.exp_name)
    save_dir = f"./results/{FLAGS.exp_name}/{FLAGS.seed}/"
    os.makedirs(save_dir, exist_ok=True)

    env, _, eval_env = make_env_and_dataset(
        FLAGS.env_name,
        FLAGS.seed,
        max_len=FLAGS.max_len,
    )
    dataset = load_replay_buffer(
        FLAGS.replay_buffer_path,  # Notice that the replay buffer also contains the offline dataset
    )

    action_dim = env.action_space.shape[0]
    max_reward = (np.max(dataset.rewards) + FLAGS.rewards_bias) * FLAGS.rewards_scale
    min_reward = (np.min(dataset.rewards) + FLAGS.rewards_bias) * FLAGS.rewards_scale
    max_q = (
        max_reward / (1 - FLAGS.config.get("discount", 0.99))
        if FLAGS.config.bound_q_functions
        else None
    )
    min_q = (
        min_reward / (1 - FLAGS.config.get("discount", 0.99))
        if FLAGS.config.bound_q_functions
        else None
    )

    for i, online_timestep in tqdm.tqdm(
        enumerate(FLAGS.online_eval_timesteps),
        smoothing=0.1,
        disable=not FLAGS.tqdm,
        desc="Training iteration",
    ):
        online_timestep = int(online_timestep)
        replay_buffer = ReplayBuffer(
            env.observation_space,
            action_dim,
            FLAGS.offline_dataset_size + online_timestep,
        )
        replay_buffer.initialize_with_dataset(
            dataset, FLAGS.offline_dataset_size + online_timestep
        )
        wandb.log(
            {
                "replay_buffer_size": replay_buffer.size,
                "replay_buffer_total_rewards": replay_buffer.rewards.sum(),
            },
            step=FLAGS.max_steps * i,
        )
        if FLAGS.config.upsample_batch_size > 0:
            upsample_replay_buffer = ReplayBuffer(
                env.observation_space,
                action_dim,
                FLAGS.offline_dataset_size + online_timestep,
            )
            if "antmaze" in env.spec.id.lower():
                reward1_idxs = np.where(
                    dataset.rewards[: FLAGS.offline_dataset_size + online_timestep] == 1
                )[0]
                upsample_dataset = Dataset(
                    observations=dataset.observations[reward1_idxs],
                    actions=dataset.actions[reward1_idxs],
                    rewards=dataset.rewards[reward1_idxs],
                    masks=dataset.masks[reward1_idxs],
                    dones_float=dataset.dones_float[reward1_idxs],
                    next_observations=dataset.next_observations[reward1_idxs],
                    size=len(reward1_idxs),
                )
            elif "kitchen" in env.spec.id.lower():
                (
                    observations,
                    actions,
                    rewards,
                    masks,
                    dones_float,
                    next_observations,
                ) = get_kitchen_successful_samples(dataset)
                upsample_dataset = Dataset(
                    observations=observations,
                    actions=actions,
                    rewards=rewards,
                    masks=masks,
                    dones_float=dones_float,
                    next_observations=next_observations,
                    size=len(rewards),
                )
            else:
                raise NotImplementedError
            upsample_replay_buffer.initialize_with_dataset(
                upsample_dataset, FLAGS.offline_dataset_size + online_timestep
            )

        exploitation_agent = Learner(
            FLAGS.seed,
            env.observation_space.sample()[np.newaxis],
            env.action_space.sample()[np.newaxis],
            minimum_q=min_q,
            maximum_q=max_q,
            use_rnd=FLAGS.use_rnd,
            intrinsic_reward_scale=FLAGS.intrinsic_reward_scale,
            **dict(FLAGS.config),
        )
        # Optionally load RND networks (only for frozen RND experiments)
        if FLAGS.load_rnd_from_path is not None:
            assert FLAGS.use_rnd, "RND path provided but not used"
            rnd_load_path = os.path.join(
                FLAGS.load_rnd_from_path, f"exploration_agent_{online_timestep}"
            )
            exploitation_agent.load_rnd_only(str(rnd_load_path))
        for exploitation_training_step in tqdm.tqdm(
            range(FLAGS.max_steps),
            disable=not FLAGS.tqdm,
            desc="Training exploitation agent",
            leave=False,
        ):
            if FLAGS.config.upsample_batch_size > 0:
                batch = replay_buffer.sample(
                    FLAGS.batch_size - FLAGS.config.upsample_batch_size
                )
                upsample_batch = upsample_replay_buffer.sample(
                    min(FLAGS.config.upsample_batch_size, upsample_replay_buffer.size)
                )
                rewards = np.concatenate((batch.rewards, upsample_batch.rewards))
                scaled_rewards = (
                    rewards + FLAGS.rewards_bias
                ) * FLAGS.rewards_scale  # scale rewards
                batch = Batch(
                    observations=np.concatenate(
                        (batch.observations, upsample_batch.observations)
                    ),
                    actions=np.concatenate((batch.actions, upsample_batch.actions)),
                    rewards=scaled_rewards,
                    masks=np.concatenate((batch.masks, upsample_batch.masks)),
                    next_observations=np.concatenate(
                        (batch.next_observations, upsample_batch.next_observations)
                    ),
                )
            else:
                batch = replay_buffer.sample(FLAGS.batch_size)
                rewards = batch.rewards
                scaled_rewards = (rewards + FLAGS.rewards_bias) * FLAGS.rewards_scale
                batch = Batch(
                    observations=batch.observations,
                    actions=batch.actions,
                    rewards=scaled_rewards,
                    masks=batch.masks,
                    next_observations=batch.next_observations,
                )

            update_info = exploitation_agent.update(batch, update_rnd=False)
            if exploitation_training_step % FLAGS.log_interval == 0:
                # Add batch info to update_info
                update_info["batch/num_positive_rewards"] = np.sum(batch.rewards == 1.0)
                if batch.weights is not None:
                    update_info["batch/weights"] = batch.weights.reshape(-1).tolist()
                for k, v in update_info.items():
                    if v.ndim == 0:
                        wandb.log(
                            {f"train/timestep_{online_timestep}_{k}": v},
                            step=FLAGS.max_steps * i + exploitation_training_step,
                        )
                    else:
                        wandb.log(
                            {
                                f"train/timestep_{online_timestep}_{k}": wandb.Histogram(
                                    v
                                )
                            },
                            step=FLAGS.max_steps * i + exploitation_training_step,
                        )
                wandb.log(
                    {
                        "train/timestep": online_timestep,
                        "train/exploitation_training_step": exploitation_training_step,
                    },
                    step=FLAGS.max_steps * i + exploitation_training_step,
                )

            if exploitation_training_step % FLAGS.eval_interval == 0:
                eval_stats = evaluate(exploitation_agent, eval_env, FLAGS.eval_episodes)
                for k, v in eval_stats.items():
                    wandb.log(
                        {f"eval/timestep_{online_timestep}_{k}": v},
                        step=FLAGS.max_steps * i + exploitation_training_step,
                    )

        # Save the model
        exploitation_agent.save_checkpoint(
            os.path.join(save_dir, f"exploitation_agent_{online_timestep}.pt")
        )
    print("Finished training.")


if __name__ == "__main__":
    app.run(main)
